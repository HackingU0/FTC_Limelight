{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true},"accelerator":"GPU","colab":{"collapsed_sections":["05N8FeXHcQp3","xmROIG9zaS9G","eGEUZYAMEZ6f","-19zML6oEO7l","kPg8oMnQDYKl","VTyqlXFTJ0Uv","XFsuasvxFHo8"],"gpuClass":"premium","gpuType":"T4","provenance":[]},"vscode":{"interpreter":{"hash":"dac6b1a68a930bf8a24417228a96ab80b19f2aa97bc2d428affc356154b4740f"}}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Install The Object Detection Package","metadata":{"id":"05N8FeXHcQp3"}},{"cell_type":"code","source":"import shutil\nimport os\ntmpModelPath ='/kaggle/working/models'\nif os.path.exists(tmpModelPath) and os.path.isdir(tmpModelPath):\n  shutil.rmtree(tmpModelPath)\n\nMLENVIRONMENT=\"KAGGLE\"\n!git clone --depth 1 https://github.com/tensorflow/models\n!cd models && git fetch --depth 1 origin ad1f7b56943998864db8f5db0706950e93bb7d81 && git checkout ad1f7b56943998864db8f5db0706950e93bb7d81\n!pip install protobuf==3.20.3","metadata":{"id":"ypWGYdPlLRUN","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:51:02.973379Z","iopub.execute_input":"2025-03-24T23:51:02.974006Z","iopub.status.idle":"2025-03-24T23:51:15.393119Z","shell.execute_reply.started":"2025-03-24T23:51:02.973929Z","shell.execute_reply":"2025-03-24T23:51:15.391700Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Environment Setup\nimport os\nimport sys\nimport re\n\nprint(sys.version)\nif(MLENVIRONMENT == \"KAGGLE\"):\n    print(\" env setup\")\n    os.environ[\"HOMEFOLDER\"] = \"/kaggle/working/\"\n    HOMEFOLDER = '{HOMEFOLDER}'.format(**os.environ)\n    FINALOUTPUTFOLDER_DIRNAME = '/kaggle/output/final_output'\n    FINALOUTPUTFOLDER = '/kaggle/output'\n    print(HOMEFOLDER)\n\n# Copy setup files into models/research folder\n!cd {HOMEFOLDER}models/research && pwd && protoc object_detection/protos/*.proto --python_out=.\n\n# Modify setup.py\nwith open(HOMEFOLDER+'models/research/object_detection/packages/tf2/setup.py') as f:\n    s = f.read()\n\nwith open(HOMEFOLDER+'models/research/setup.py', 'w') as f:\n    if(MLENVIRONMENT == \"KAGGLE\"):\n        s = re.sub('tf-models-official>=2.5.1','tf-models-official==2.15.0', s)\n        f.write(s)","metadata":{"id":"6QPmVBSlLTzM","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:51:15.395044Z","iopub.execute_input":"2025-03-24T23:51:15.395506Z","iopub.status.idle":"2025-03-24T23:51:15.597231Z","shell.execute_reply.started":"2025-03-24T23:51:15.395456Z","shell.execute_reply":"2025-03-24T23:51:15.595761Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install\n!pip install {HOMEFOLDER}models/research/\nif(MLENVIRONMENT == \"KAGGLE\"):\n    !pip install tensorflow==2.15.0\n    !pip install protobuf==3.20.3","metadata":{"id":"OLDnCkLLwLr6","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:51:18.652697Z","iopub.execute_input":"2025-03-24T23:51:18.653125Z","iopub.status.idle":"2025-03-24T23:51:37.442426Z","shell.execute_reply.started":"2025-03-24T23:51:18.653087Z","shell.execute_reply":"2025-03-24T23:51:37.440930Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Test the environment by running `model_builder_tf2_test.py` to make sure everything is working as expected.","metadata":{"id":"6V7TrfUos-9E"}},{"cell_type":"code","source":"!python {HOMEFOLDER}models/research/object_detection/builders/model_builder_tf2_test.py","metadata":{"id":"wh_HPMOqWH9z","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:51:41.980625Z","iopub.execute_input":"2025-03-24T23:51:41.981012Z","iopub.status.idle":"2025-03-24T23:51:43.281408Z","shell.execute_reply.started":"2025-03-24T23:51:41.980946Z","shell.execute_reply":"2025-03-24T23:51:43.280199Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 1.1. Get Dataset From Google Drive\n\n1. Expand this section\n2. Upload your RoboFlow .tfrecord.zip to Google Drive\n3. Share the uploaded .tfrecord.zip such that anyone with the link can access the file.\n4. Run this block\n5. Paste your Google Drive file share link into the text box that appears after running this block\n6. Click the \"Process Dataset\" Buttton\n7. Click the Refresh button in the \"Files\" pane to ensure dataset.zip exists","metadata":{"id":"xmROIG9zaS9G"}},{"cell_type":"code","source":"import gdown\nimport os\n\ndef download_dataset():\n    try:\n        print(\"Downloading dataset...\")\n        url = 'https://drive.google.com/uc?id=1A6ac-3qvxdBPYP6ARiIG2cu9j3FFCUdH'\n        output = '/kaggle/working/dataset.zip'\n        gdown.download(url, output, fuzzy=True)\n        print(\"Download complete!\")\n    except Exception as e:\n        print(f\"Error: {str(e)}\")\n\n# Install gdown if not already installed\n!pip install -q gdown --upgrade\n\n# Execute\ndownload_dataset()","metadata":{"id":"tLgAPsQsfTLs","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:58:47.549205Z","iopub.execute_input":"2025-03-24T23:58:47.549546Z","iopub.status.idle":"2025-03-24T23:58:59.702432Z","shell.execute_reply.started":"2025-03-24T23:58:47.549520Z","shell.execute_reply":"2025-03-24T23:58:59.701194Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 2. Auto-detect relevant tfrecord components","metadata":{"id":"m6kMXxVJo5za"}},{"cell_type":"code","source":"datasetPath = '/kaggle/working/dataset.zip'\nprint(datasetPath)\n!unzip $datasetPath","metadata":{"id":"n9bd6VsJf2wl","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:52:02.201495Z","iopub.execute_input":"2025-03-24T23:52:02.202030Z","iopub.status.idle":"2025-03-24T23:52:02.814793Z","shell.execute_reply.started":"2025-03-24T23:52:02.201989Z","shell.execute_reply":"2025-03-24T23:52:02.813522Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport fnmatch\n\ndef find_files(directory, pattern):\n    for root, dirs, files in os.walk(directory):\n        for basename in files:\n            if fnmatch.fnmatch(basename, pattern):\n                filename = os.path.join(root, basename)\n                yield filename\n\ndef set_tfrecord_variables(directory):\n    train_record_fname = ''\n    val_record_fname = ''\n    label_map_pbtxt_fname = ''\n\n    for tfrecord_file in find_files(directory, '*.tfrecord'):\n        if '/train/' in tfrecord_file:\n            train_record_fname = tfrecord_file\n        elif '/valid/' in tfrecord_file:\n            val_record_fname = tfrecord_file\n        elif '/test/' in tfrecord_file:\n            pass\n\n    for label_map_file in find_files(directory, '*_label_map.pbtxt'):\n        label_map_pbtxt_fname = label_map_file  # Assuming one common label map file\n\n    return train_record_fname, val_record_fname, label_map_pbtxt_fname\n\n\ntrain_record_fname, val_record_fname, label_map_pbtxt_fname = set_tfrecord_variables('/kaggle/working')\n\n#if(MLENVIRONMENT==\"KAGGLE\"):\n    #train_record_fname = '/content/train/cubes-cones.tfrecord'\n    #val_record_fname = '/content/valid/cubes-cones.tfrecord'\n    #label_map_pbtxt_fname = '/content/train/cubes-cones_label_map.pbtxt'\n\nprint(\"Train Record File:\", train_record_fname)\nprint(\"Validation Record File:\", val_record_fname)\nprint(\"Label Map File:\", label_map_pbtxt_fname)\n\n","metadata":{"id":"YUd2wtfrqedy","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:52:14.673943Z","iopub.execute_input":"2025-03-24T23:52:14.674443Z","iopub.status.idle":"2025-03-24T23:52:14.733489Z","shell.execute_reply.started":"2025-03-24T23:52:14.674353Z","shell.execute_reply":"2025-03-24T23:52:14.732395Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 3.&nbsp;Training Configuration and Labels File Generation","metadata":{"id":"eGEUZYAMEZ6f"}},{"cell_type":"markdown","source":"Download the pre-trained Limelight Base Model","metadata":{"id":"I2MAcgJ53STW"}},{"cell_type":"code","source":"chosen_model = 'ssd-mobilenet-v2'\nMODELS_CONFIG = {\n    'ssd-mobilenet-v2': {\n        'model_name': 'ssd_mobilenet_v2_320x320_coco17_tpu-8',\n        'base_pipeline_file': 'limelight_ssd_mobilenet_v2_320x320_coco17_tpu-8.config',\n        'pretrained_checkpoint': 'limelight_ssd_mobilenet_v2_320x320_coco17_tpu-8.tar.gz',\n    },\n}\nmodel_name = MODELS_CONFIG[chosen_model]['model_name']\npretrained_checkpoint = MODELS_CONFIG[chosen_model]['pretrained_checkpoint']\nbase_pipeline_file = MODELS_CONFIG[chosen_model]['base_pipeline_file']\n\n# Create \"mymodel\" folder for pre-trained weights and configuration files\n%cd ~\n%mkdir {HOMEFOLDER}models/mymodel/\n%cd {HOMEFOLDER}models/mymodel/\n%pwd\n\n# Download pre-trained model weights\nimport tarfile\ndownload_tar = 'https://downloads.limelightvision.io/models/' + pretrained_checkpoint\n!wget {download_tar}\ntar = tarfile.open(pretrained_checkpoint)\ntar.extractall()\ntar.close()\n\n# Download training configuration file for model\ndownload_config = 'https://downloads.limelightvision.io/models/' + base_pipeline_file\n!wget {download_config}\n%cd ~\n\n# Set training parameters for the model\nnum_steps = 40000\ncheckpoint_every = 2000\nbatch_size = 16\n","metadata":{"id":"gN0EUEa3e5Un","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:52:24.295515Z","iopub.execute_input":"2025-03-24T23:52:24.295920Z","iopub.status.idle":"2025-03-24T23:52:26.624146Z","shell.execute_reply.started":"2025-03-24T23:52:24.295886Z","shell.execute_reply":"2025-03-24T23:52:26.622773Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Generate Labels File","metadata":{"id":"XMbr89qqgTVW"}},{"cell_type":"code","source":"\n# Set file locations and get number of classes for config file\npipeline_fname = HOMEFOLDER+'models/mymodel/' + base_pipeline_file\nfine_tune_checkpoint = HOMEFOLDER+'models/mymodel/' + model_name + '/checkpoint/ckpt-0'\n\ndef get_num_classes(pbtxt_fname):\n    from object_detection.utils import label_map_util\n    label_map = label_map_util.load_labelmap(pbtxt_fname)\n    categories = label_map_util.convert_label_map_to_categories(\n        label_map, max_num_classes=90, use_display_name=True)\n    category_index = label_map_util.create_category_index(categories)\n    return len(category_index.keys())\n\ndef get_classes(pbtxt_fname):\n    from object_detection.utils import label_map_util\n    label_map = label_map_util.load_labelmap(pbtxt_fname)\n    categories = label_map_util.convert_label_map_to_categories(\n        label_map, max_num_classes=90, use_display_name=True)\n    category_index = label_map_util.create_category_index(categories)\n\n    class_names = [category['name'] for category in category_index.values()]\n    return class_names\n\ndef create_label_file(filename, labels):\n    with open(filename, 'w') as file:\n        for label in labels:\n            file.write(label + '\\n')\n\n\nnum_classes = get_num_classes(label_map_pbtxt_fname)\nclasses = get_classes(label_map_pbtxt_fname)\n\nprint('Total classes:', num_classes)\nprint(classes)\n\n\n#Generate labels file\ncreate_label_file(HOMEFOLDER + \"limelight_neural_detector_labels.txt\", classes)","metadata":{"id":"DDyH_i3MgP1D","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:52:31.633359Z","iopub.execute_input":"2025-03-24T23:52:31.633782Z","iopub.status.idle":"2025-03-24T23:52:38.931895Z","shell.execute_reply.started":"2025-03-24T23:52:31.633748Z","shell.execute_reply":"2025-03-24T23:52:38.930748Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Modify the base Limelight Model Configuration File\n\nAugmentation Options: https://github.com/tensorflow/models/blob/master/research/object_detection/protos/preprocessor.proto","metadata":{"id":"cwPyaIAXxyKu"}},{"cell_type":"code","source":"# Create custom configuration file by writing the dataset, model checkpoint, and training parameters into the base pipeline file\nimport re\n\nprint('writing custom configuration file')\n\n\n\nwith open(pipeline_fname) as f:\n    s = f.read()\nwith open('pipeline_file.config', 'w') as f:\n\n    # Set fine_tune_checkpoint path\n    s = re.sub('fine_tune_checkpoint: \".*?\"',\n               'fine_tune_checkpoint: \"{}\"'.format(fine_tune_checkpoint), s)\n\n    # Set tfrecord files for train and test datasets\n    s = re.sub(\n        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/train)(.*?\")', 'input_path: \"{}\"'.format(train_record_fname), s)\n    s = re.sub(\n        '(input_path: \".*?)(PATH_TO_BE_CONFIGURED/val)(.*?\")', 'input_path: \"{}\"'.format(val_record_fname), s)\n\n    # Set label_map_path\n    s = re.sub(\n        'label_map_path: \".*?\"', 'label_map_path: \"{}\"'.format(label_map_pbtxt_fname), s)\n\n    # Set batch_size\n    s = re.sub('batch_size: [0-9]+',\n               'batch_size: {}'.format(batch_size), s)\n\n    # Set training steps, num_steps\n    s = re.sub('num_steps: [0-9]+',\n               'num_steps: {}'.format(num_steps), s)\n\n    # Set number of classes num_classes\n    s = re.sub('checkpoint_every_n: [0-9]+',\n               'num_classes: {}'.format(num_classes), s)\n\n    # Change fine-tune checkpoint type from \"classification\" to \"detection\"\n    s = re.sub(\n        'fine_tune_checkpoint_type: \"classification\"', 'fine_tune_checkpoint_type: \"{}\"'.format('detection'), s)\n\n    # If using ssd-mobilenet-v2, reduce learning rate\n    if chosen_model == 'ssd-mobilenet-v2':\n      s = re.sub('learning_rate_base: .8',\n                 'learning_rate_base: .004', s)\n\n      s = re.sub('warmup_learning_rate: 0.13333',\n                 'warmup_learning_rate: .0016666', s)\n\n    # If using efficientdet-d0, use fixed_shape_resizer instead of keep_aspect_ratio_resizer (because it isn't supported by TFLite)\n    if chosen_model == 'efficientdet-d0':\n      s = re.sub('keep_aspect_ratio_resizer', 'fixed_shape_resizer', s)\n      s = re.sub('pad_to_max_dimension: true', '', s)\n      s = re.sub('min_dimension', 'height', s)\n      s = re.sub('max_dimension', 'width', s)\n\n    f.write(s)\n\n# (Optional) Display the custom configuration file's contents\n# !cat pipeline_file.config\n# Set the path to the custom config file and the directory to store training checkpoints in\npipeline_file = 'pipeline_file.config'\nmodel_dir = HOMEFOLDER+'training_progress/'\nprint(\" \")\nprint(model_dir)","metadata":{"id":"5eA5ht3_yukT","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:53:05.384996Z","iopub.execute_input":"2025-03-24T23:53:05.385906Z","iopub.status.idle":"2025-03-24T23:53:05.401221Z","shell.execute_reply.started":"2025-03-24T23:53:05.385866Z","shell.execute_reply":"2025-03-24T23:53:05.399997Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4.&nbsp;Train Model","metadata":{"id":"-19zML6oEO7l"}},{"cell_type":"markdown","source":"Once training starts, come back and click the refresh button within the tensorboard window to check training progress.\n\n","metadata":{"id":"XxPj_QV43qD5"}},{"cell_type":"code","source":"%load_ext tensorboard\n%tensorboard --logdir '/kaggle/working/training_progress/train'","metadata":{"id":"TI9iCCxoNlAL","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T23:53:09.531743Z","iopub.execute_input":"2025-03-24T23:53:09.532151Z","iopub.status.idle":"2025-03-24T23:53:17.086833Z","shell.execute_reply.started":"2025-03-24T23:53:09.532119Z","shell.execute_reply":"2025-03-24T23:53:17.085400Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fix TF 2.15 breaking changes","metadata":{"id":"ejo07C1zXHzY"}},{"cell_type":"code","source":"!pip install tf_slim --upgrade\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-24T19:19:44.286871Z","iopub.execute_input":"2025-03-24T19:19:44.287263Z","iopub.status.idle":"2025-03-24T19:19:48.714885Z","shell.execute_reply.started":"2025-03-24T19:19:44.287233Z","shell.execute_reply":"2025-03-24T19:19:48.713460Z"},"id":"k6OTOB6r-tIj"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shutil\nimport re\n\noriginal_path = '/usr/local/lib/python3.10/dist-packages/tf_slim/data/tfexample_decoder.py'\nwith open(original_path, 'r') as file:\n  content = file.read()\n  content = re.sub(r'import abc', 'import tensorflow as tf\\n\\nimport abc', content)\n  content = re.sub(r'control_flow_ops.case', 'tf.case', content)\n  content = re.sub(r'control_flow_ops.cond', 'tf.compat.v1.cond', content)\nwith open(original_path, 'w') as file:\n  file.write(content)\n\nprint(f\"File {original_path} fixed.\")","metadata":{"id":"Ltvi224axv3Y","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T19:20:17.003969Z","iopub.execute_input":"2025-03-24T19:20:17.004391Z","iopub.status.idle":"2025-03-24T19:20:17.013282Z","shell.execute_reply.started":"2025-03-24T19:20:17.004348Z","shell.execute_reply":"2025-03-24T19:20:17.012338Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Train","metadata":{"id":"AjqYo9r9ffVx"}},{"cell_type":"code","source":"!rm -rf {HOMEFOLDER}training_progress\n# Run training!\n!python {HOMEFOLDER}models/research/object_detection/model_main_tf2.py \\\n    --pipeline_config_path={pipeline_file} \\\n    --model_dir={model_dir} \\\n    --alsologtostderr \\\n    --checkpoint_every_n={checkpoint_every} \\\n    --num_train_steps={num_steps} \\\n    --num_workers=2 \\\n    --sample_1_of_n_eval_examples=1","metadata":{"id":"tQTfZChVzzpZ","trusted":true,"execution":{"iopub.status.busy":"2025-03-24T19:20:25.493181Z","iopub.execute_input":"2025-03-24T19:20:25.493536Z","iopub.status.idle":"2025-03-24T19:20:33.986084Z","shell.execute_reply.started":"2025-03-24T19:20:25.493510Z","shell.execute_reply":"2025-03-24T19:20:33.984634Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Feel free to stop training early. Check the 'training_progress' folder to see all training checkpoints.\n","metadata":{"id":"WHxbX4ZpzXIv"}},{"cell_type":"markdown","source":"# 5.&nbsp;Convert Model to TFLite","metadata":{"id":"kPg8oMnQDYKl"}},{"cell_type":"code","source":"#remove final output folder if it exists\nif os.path.exists(FINALOUTPUTFOLDER) and os.path.isdir(FINALOUTPUTFOLDER):\n  shutil.rmtree(FINALOUTPUTFOLDER)\n\n# Make a directory to store the trained TFLite model\n!mkdir {FINALOUTPUTFOLDER}\nprint(FINALOUTPUTFOLDER)\n# Export graph\n# Path to training directory (the conversion script automatically chooses the highest checkpoint file)\nlast_model_path = HOMEFOLDER+'training_progress'\nexporter_path = HOMEFOLDER+'models/research/object_detection/export_tflite_graph_tf2.py'\noutput_directory = FINALOUTPUTFOLDER\n\n!python $exporter_path \\\n    --trained_checkpoint_dir $last_model_path \\\n    --output_directory $output_directory \\\n    --pipeline_config_path $pipeline_file\n\n# Convert to .tflite Flatbuffer\nimport tensorflow as tf\n\nconverter = tf.lite.TFLiteConverter.from_saved_model(FINALOUTPUTFOLDER+'/saved_model')\ntflite_model = converter.convert()\nmodel_path_32bit = FINALOUTPUTFOLDER+'/limelight_neural_detector_32bit.tflite'\nwith open(model_path_32bit, 'wb') as f:\n  f.write(tflite_model)\n\n!cp {HOMEFOLDER}limelight_neural_detector_labels.txt {FINALOUTPUTFOLDER}\n!cp {HOMEFOLDER}models/mymodel/pipeline_file.config {FINALOUTPUTFOLDER}","metadata":{"id":"RaUU8tBlHifd"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Export graph\n# Path to training directory (the conversion script automatically chooses the highest checkpoint file)\nlast_model_path = HOMEFOLDER+'training_progress'\nexporter_path = HOMEFOLDER+'models/research/object_detection/export_tflite_graph_tf2.py'\noutput_directory = FINALOUTPUTFOLDER\n\n!python $exporter_path \\\n    --trained_checkpoint_dir $last_model_path \\\n    --output_directory $output_directory \\\n    --pipeline_config_path $pipeline_file\n\n# Convert to .tflite Flatbuffer\nimport tensorflow as tf\n\nconverter = tf.lite.TFLiteConverter.from_saved_model(FINALOUTPUTFOLDER+'/saved_model')\ntflite_model = converter.convert()\nmodel_path_32bit = FINALOUTPUTFOLDER+'/limelight_neural_detector_32bit.tflite'\nwith open(model_path_32bit, 'wb') as f:\n  f.write(tflite_model)\n\n!cp {HOMEFOLDER}limelight_neural_detector_labels.txt {FINALOUTPUTFOLDER}","metadata":{"id":"gqahbHU1suBi"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. Quantize model\nThe \"TFLiteConverter\" module will perform [post-training quantization](https://www.tensorflow.org/lite/performance/post_training_quantization) on the model. To quantize the model, we need to provide a set of example images. We will extract 100 images from the training tfrecord and place said images into the \"extracted_samples\" folder.\n","metadata":{"id":"VTyqlXFTJ0Uv"}},{"cell_type":"code","source":"import tensorflow as tf\nimport os\nimport io\nfrom PIL import Image\n\ndef extract_images_from_tfrecord(tfrecord_path, output_folder, num_samples=100):\n    # Make sure the output directory exists\n    if not os.path.exists(output_folder):\n        os.makedirs(output_folder)\n\n    # Initialize a counter for the number of images saved\n    saved_images = 0\n\n    # Read the TFRecord file\n    raw_dataset = tf.data.TFRecordDataset(tfrecord_path)\n    for raw_record in raw_dataset.take(num_samples):\n        example = tf.train.Example()\n        example.ParseFromString(raw_record.numpy())\n\n        # Extract the image data (change 'image/encoded' if necessary)\n        image_data = example.features.feature['image/encoded'].bytes_list.value[0]\n\n        # Decode the image data and save as a file\n        image = Image.open(io.BytesIO(image_data))\n        image.save(os.path.join(output_folder, f'image_{saved_images}.png'))\n\n        saved_images += 1\n        if saved_images >= num_samples:\n            break\n\n    print(f\"Extracted {saved_images} images to {output_folder}\")\n\n# Set the path to your TFRecord file and the output directory\ntfrecord_path = train_record_fname\nextracted_sample_folder = HOMEFOLDER+'extracted_samples'\n\n#remove sample folder if it exists\nif os.path.exists(extracted_sample_folder) and os.path.isdir(extracted_sample_folder):\n  shutil.rmtree(extracted_sample_folder)\n\n# Extract images\nextract_images_from_tfrecord(tfrecord_path, extracted_sample_folder)\n\n\n# Get list of all images in train directory\nfrom google.cloud import storage\nimport glob\n\nquant_image_list=[]\nif(MLENVIRONMENT==\"KAGGLE\"):\n\n    jpg_file_list = glob.glob(extracted_sample_folder + '/*.jpg')\n    jpeg_file_list = glob.glob(extracted_sample_folder + '/*.jpeg')\n    JPG_file_list = glob.glob(extracted_sample_folder + '/*.JPG')\n    png_file_list = glob.glob(extracted_sample_folder + '/*.png')\n    bmp_file_list = glob.glob(extracted_sample_folder + '/*.bmp')\n    quant_image_list = jpg_file_list + JPG_file_list + png_file_list + bmp_file_list\n\nprint(\"pulling samples from \" + extracted_sample_folder)\nprint(\"samples: \" + str(len(quant_image_list)))","metadata":{"id":"XSNZtfj_k3NP"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# A generator that provides a representative dataset\n# Code modified from https://colab.research.google.com/github/google-coral/tutorials/blob/master/retrain_classification_ptq_tf2.ipynb\n\n# First, get input details for model so we know how to preprocess images\ninterpreter = tf.lite.Interpreter(model_path=model_path_32bit)\ninterpreter.allocate_tensors()\ninput_details = interpreter.get_input_details()\noutput_details = interpreter.get_output_details()\nheight = input_details[0]['shape'][1]\nwidth = input_details[0]['shape'][2]\n\nimport random\n\ndef representative_data_gen():\n  dataset_list = quant_image_list\n  quant_num = 300\n  for i in range(quant_num):\n    pick_me = random.choice(dataset_list)\n    print(pick_me)\n    image = tf.io.read_file(pick_me)\n\n    if pick_me.endswith('.jpg') or pick_me.endswith('.JPG') or pick_me.endswith('.jpeg'):\n      image = tf.io.decode_jpeg(image, channels=3)\n    elif pick_me.endswith('.png'):\n      image = tf.io.decode_png(image, channels=3)\n    elif pick_me.endswith('.bmp'):\n      image = tf.io.decode_bmp(image, channels=3)\n\n    image = tf.image.resize(image, [width, height])  # TO DO: Replace 300s with an automatic way of reading network input size\n    image = tf.cast(image / 255., tf.float32)\n    image = tf.expand_dims(image, 0)\n    yield [image]","metadata":{"id":"ORzx0XRErSLV"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Finally, we'll initialize the TFLiteConverter module, point it at the TFLite graph we generated in Step 6, and provide it with the representative dataset generator function we created in the previous code block. We'll configure the converter to quantize the model's weight values to INT8 format.","metadata":{"id":"wqtu98mzebEj"}},{"cell_type":"code","source":"# Initialize converter module\nconverter = tf.lite.TFLiteConverter.from_saved_model(FINALOUTPUTFOLDER+'/saved_model')\nprint(\"initialized converter\")\n# This enables quantization\nconverter.optimizations = [tf.lite.Optimize.DEFAULT]\n# This sets the representative dataset for quantization\nconverter.representative_dataset = representative_data_gen\n# This ensures that if any ops can't be quantized, the converter throws an error\nconverter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n# For full integer quantization, though supported types defaults to int8 only, we explicitly declare it for clarity.\nconverter.target_spec.supported_types = [tf.int8]\n# These set the input tensors to uint8 and output tensors to float32\nconverter.inference_input_type = tf.uint8\nconverter.inference_output_type = tf.float32\nprint(\"begin conversion\")\ntflite_model = converter.convert()\nprint(\"conversion complete\")\n\nwith open(FINALOUTPUTFOLDER+'/limelight_neural_detector_8bit.tflite', 'wb') as f:\n  f.write(tflite_model)","metadata":{"id":"Ox0bGDWds_Ce"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7. Compile Model for Limelight & Download\n","metadata":{"id":"XFsuasvxFHo8"}},{"cell_type":"markdown","source":"Install Coral Compiler","metadata":{"id":"peawOI_z0DHt"}},{"cell_type":"code","source":"! curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n! echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n! sudo apt-get update\n! sudo apt-get install edgetpu-compiler","metadata":{"id":"mUd_SNC0JSq0"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Compile the previously-generated 8-bit model for Google Coral","metadata":{"id":"usfmdtSiJuuC"}},{"cell_type":"code","source":"!cd {FINALOUTPUTFOLDER} && pwd && edgetpu_compiler limelight_neural_detector_8bit.tflite && pwd && mv limelight_neural_detector_8bit_edgetpu.tflite limelight_neural_detector_coral.tflite && rm limelight_neural_detector_8bit_edgetpu.log","metadata":{"id":"mULCY0nb0ahH"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Zip models","metadata":{"id":"oqGy2FgzKomN"}},{"cell_type":"code","source":"!rm {HOMEFOLDER}limelight_detectors.zip\n!zip -r {HOMEFOLDER}limelight_detectors.zip /kaggle/output","metadata":{"id":"8nCdUouYJjQM"},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Download","metadata":{"id":"vHgbpkQue-ZR"}}]}